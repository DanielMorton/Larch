{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Larch Edet1.ipynb","provenance":[{"file_id":"136CHhNxCdgrC_07qLAQheNbHTzfbhTS7","timestamp":1631647169417},{"file_id":"1U_iIycTGCFETwWk96zlIeBOK8dYwZfOe","timestamp":1628793877248},{"file_id":"13JOLcC5CVkCyOU5-C6sxXB3Tgwz6N-K9","timestamp":1626320199556},{"file_id":"1-W6RkAKnQBIwAcwVM2geWApOtx_bYUu_","timestamp":1626116656370},{"file_id":"1lPGnvUeovFTrFxOR5ABj8de_EuQ239jF","timestamp":1626115779430}],"collapsed_sections":[],"mount_file_id":"136CHhNxCdgrC_07qLAQheNbHTzfbhTS7","authorship_tag":"ABX9TyMuSR2NoP1pgR0tZXYhl0+Y"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"fx7mmFKH0oql"},"source":["## **Load Packages and Set Environment Variables**"]},{"cell_type":"code","metadata":{"id":"ScNeyugmwaob"},"source":["import numpy as np\n","import pandas as pd\n","\n","import os\n","import cv2\n","import matplotlib.pyplot as plt\n","from glob import glob\n","from sklearn.model_selection import train_test_split\n","\n","import time\n","import torch\n","import zipfile\n","import xml.etree.ElementTree as ET\n","from torch.utils.data import Dataset\n","from torch.utils.data.sampler import SequentialSampler, RandomSampler\n","from datetime import datetime\n","\n","\n","DRIVE = \"/content/Larch\"\n","IMAGE_ZIP = \"Data_Set_Larch_Casebearer.zip\"\n","IMAGES = \"Data_Set_Larch_Casebearer\"\n","os.environ[\"DRIVE\"] = DRIVE\n","os.environ[\"DRIVE_ZIP\"] = f\"{DRIVE}/{IMAGE_ZIP}\"\n","os.environ[\"IMAGE_ZIP\"] = IMAGE_ZIP\n","os.environ[\"IMAGES\"] = IMAGES\n","\n","BASE_DIR=DRIVE\n","\n","IMG_SIZE = [512, 640, 768, 896, 1024, 1280, 1280, 1536]\n","BATCH_SIZE = 2\n","D_SIZE = 1  # Model Size\n","C_SIZE = 1  # Image Size"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"av8qHh87002C"},"source":["## **Load Data and Install Petrel**\n","\n","Petrel streamlines the model pipeline."]},{"cell_type":"code","metadata":{"id":"-pWLNcuawuRO"},"source":["%%bash\n","wget https://lilablobssc.blob.core.windows.net/larch-casebearer/$IMAGE_ZIP\n","unzip -q /content/$IMAGE_ZIP\n","rm /content/$IMAGE_ZIP\n","\n","pip install -U -q albumentations\n","pip install -q petrel-det"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MlUhebJM1F3f"},"source":["import albumentations as A\n","from albumentations.pytorch.transforms import ToTensorV2\n","\n","from petrel.dataset import TrainDataset, ValDataset\n","from petrel.model import load_edet, load_optimizer, load_scheduler, ModelTrainer\n","\n","## Bounding Box Parameters.\n","BBOX = A.BboxParams(format='pascal_voc',\n","                    min_area=0,\n","                    min_visibility=0,\n","                    label_fields=['labels'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JisFP2WMDkJS"},"source":["def xml_to_df(d, directory, filename):\n","    xml_split = filename.split('/')\n","    file = xml_split[-1].split('.')[0]\n","    location, date = directory.split(\"/\")[1].split('_')\n","    root = ET.XML(d)\n","    objects = []\n","    for child in root:\n","        if child.tag == 'object':\n","            objects.append(child)\n","        elif child.tag == 'size':\n","            dims = {s.tag: int(s.text) for s in child}\n","    \n","    datas = []\n","    for child in objects:\n","        data ={}\n","        for c in child:\n","            if c.tag != 'bndbox':\n","                data[c.tag] = [c.text]\n","            else:\n","                for b in c:\n","                    data[b.tag] = [int(b.text)]\n","        datas.append(pd.DataFrame(data))\n","    try:\n","        df = pd.concat(datas)\n","        df['height'], df['width'] = dims['height'], dims['width']\n","        df['location'], df['date'], df['file_name'] = location, int(date), file\n","    except:\n","        print(filename)\n","        df = pd.DataFrame({'height': dims['height'],\n","                           'width': dims['width'],\n","                           'location': [location],\n","                           'date': [int(date)],\n","                           'file_name': [file]})\n","    df[\"file\"] = df.apply(lambda row: f\"{IMAGES}/{row['location']}_{row['date']}/Images/{row['file_name']}.JPG\", axis=1)\n","    \n","    return df\n","\n","def read_xml(xml_dir):\n","    df_list = []\n","    for filename in [f\"{IMAGES}/{xml_dir}/Annotations/{f}\" for  f in os.listdir(f\"{IMAGES}/{xml_dir}/Annotations\") if \"__\" not in f and \".xml\" in f]:\n","        with open(filename) as f:\n","            d = f.read() \n","            df_list.append(xml_to_df(d, f\"{IMAGES}/{xml_dir}/Annotations\", filename))\n","    \n","    return pd.concat(df_list).reset_index(drop=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1ttIwejoBMk8"},"source":["# **Process XML metadata to DataFrame**"]},{"cell_type":"code","metadata":{"id":"NHPsfoHHw4s0"},"source":["%%time\n","def get_meta_data():\n","  larch_dirs = [f for f in os.listdir(IMAGES)]\n","  image_cols = [\"location\", \"file\", \"file_name\", \"height\", \"width\"]\n","  box_cols = [\"tree\", \"damage\", \"labels\", \"xmin\", \"ymin\", \"xmax\", \"ymax\", \"file\"]\n","\n","  ## Concatenate Dataframes from each location into single Dataframe.\n","  xml_df = pd.concat([read_xml(ld) for ld in larch_dirs]).reset_index(drop=True)\n","  xml_df['truncated'] = xml_df['truncated'].astype(float)\n","  \n","  ## Standardize column names.\n","  xml_df.loc[xml_df[~xml_df['name'].isna()].index, 'tree'] = xml_df.loc[xml_df[~xml_df['name'].isna()].index, 'name']\n","\n","  ## Drop redundant columns.\n","  xml_df.drop(columns=['name', 'difficult', 'pose'], inplace=True)\n","\n","  ## Standardize label for detection of other tree species.\n","  xml_df.loc[xml_df[xml_df['tree'].isna()].index, 'tree'] = 'Other'\n","  xml_df['tree'] = xml_df['tree'].apply(lambda t: t.capitalize()).apply(lambda t: t if t != 'Spruce' else 'Other')\n","\n","  ## Remove detections with no damage information.\n","  xml_df = xml_df[~xml_df['damage'].isnull()].reset_index(drop=True)\n","  xml_df['truncated'] = xml_df['truncated'].astype(int)\n","  for col in [\"truncated\", \"xmin\", \"xmax\", \"ymin\", \"ymax\"]:\n","    xml_df[col] = xml_df[col].astype(int)\n","  damage_map = {d: n + 1 for n, d in enumerate(xml_df[\"damage\"].sort_values().unique())}\n","  xml_df[\"labels\"] = xml_df[\"damage\"].apply(lambda d: damage_map[d])\n","  xml_df = xml_df[xml_df['file_name'] != \"B01_0023\"]\n","\n","  ## Drop duplicate entries.\n","  xml_df = xml_df.drop_duplicates().reset_index(drop=True)\n","\n","  ## Remove August Data.\n","  xml_images = xml_df[xml_df['date'] == 20190527][image_cols].drop_duplicates().reset_index(drop=True)\n","  xml_boxes = xml_df[xml_df['date'] == 20190527][box_cols].reset_index(drop=True)\n","  xml_boxes = xml_boxes[xml_boxes[\"xmin\"] != xml_boxes[\"xmax\"]].reset_index(drop=True)\n","\n","  # Train-Val Split\n","  xml_train, xml_val = train_test_split(xml_images,\n","                                        test_size=0.2,\n","                                        random_state=64,\n","                                        stratify=xml_images['location'])\n","  xml_train.reset_index(drop=True, inplace=True)\n","  xml_val.reset_index(drop=True, inplace=True)\n","  xml_train_boxes = xml_boxes[xml_boxes[\"file\"].isin(xml_train[\"file\"])].reset_index(drop=True)\n","  xml_val_boxes = xml_boxes[xml_boxes[\"file\"].isin(xml_val[\"file\"])].reset_index(drop=True)\n","  return xml_train, xml_val, xml_train_boxes, xml_val_boxes\n","\n","xml_train, xml_val, xml_train_boxes, xml_val_boxes = get_meta_data()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TzzRsXb_BaVR"},"source":["**Training Preprocessing**\n","\n","Random crop.\n","\n","Random horizontal and vertical flips.\n","Random rotation and transpose.\n","\n","**Validation Preprocessing**\n","\n","Resize"]},{"cell_type":"code","metadata":{"id":"YcY-Ugey04p8"},"source":["def get_train_transforms(d_size):\n","    \"\"\"\n","    Returns a function to perform the standard sequence of preprocessing steps\n","    for training data.\n","    \"\"\"\n","    return A.Compose([A.RandomResizedCrop(height=IMG_SIZE[d_size],\n","                                          width=IMG_SIZE[d_size],\n","                                          scale=(0.05, 1),\n","                                          ratio=(3/4, 4/3),\n","                                          p=1.0),\n","                      A.HorizontalFlip(p=0.5),\n","                      A.VerticalFlip(p=0.5),\n","                      A.RandomRotate90(p=1.0),\n","                      A.Transpose(p=0.5),\n","                      ToTensorV2(p=1.0)],\n","                     bbox_params=BBOX,\n","                     p=1.0)\n","\n","def get_val_full_transform(d_size):\n","    \"\"\"\n","    Returns a function to perform the standard sequence of preprocessing steps\n","    for validation data.\n","    \"\"\"\n","    return A.Compose([A.Resize(height=IMG_SIZE[d_size],\n","                               width=IMG_SIZE[d_size],\n","                               p=1.0),\n","                      ToTensorV2(p=1.0)],\n","                     bbox_params=BBOX,\n","                     p=1.0)\n","\n","def collate_fn(batch):\n","    return tuple(zip(*batch))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xz1SnIkW070W"},"source":["train_dataset = TrainDataset(meta_data=xml_train,\n","                             boxes=xml_train_boxes,\n","                             image_root=\"/content\",\n","                             transform=get_train_transforms(d_size=D_SIZE))\n","train_loader = torch.utils.data.DataLoader(\n","    train_dataset,\n","    batch_size=BATCH_SIZE,\n","    sampler=RandomSampler(train_dataset),\n","    pin_memory=False,\n","    drop_last=True,\n","    collate_fn=collate_fn)\n","\n","val_full_dataset = ValDataset(\n","    meta_data=xml_val,\n","    boxes=xml_val_boxes,\n","    image_root=\"/content\",\n","    transform=get_val_full_transform(d_size=D_SIZE),\n","    train_pipe=True\n",")\n","\n","val_full_loader = torch.utils.data.DataLoader(\n","    val_full_dataset, \n","    batch_size=BATCH_SIZE,\n","    sampler=SequentialSampler(val_full_dataset),\n","    shuffle=False,\n","    pin_memory=False,\n","    collate_fn=collate_fn)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K9GP3wyOB1G6"},"source":["**Set up model**\n","\n","200 Epochs\n","\n","Initial learning rate 0.000256\n","\n","Cosine decay to 0 over all Epochs."]},{"cell_type":"code","metadata":{"id":"rS9v_s3Kx5jC"},"source":["N = 200\n","model = load_edet(f\"tf_efficientdet_d{D_SIZE}\", image_size=IMG_SIZE[D_SIZE],\n","                  num_classes=4)\n","optimizer = load_optimizer(\"adamw\", model, learning_rate=2.56e-4)\n","scheduler = load_scheduler(\"cosine\", optimizer=optimizer, T_max=N)\n","model_trainer = ModelTrainer(model, optimizer, scheduler,\n","                             base_dir=f\"{DRIVE}/effdet{D_SIZE}_petrel_cosine256e4_bs4\",\n","                             verbose_step=20,\n","                             num_epochs=N)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FeZEtOoH9RLh"},"source":["model_trainer.fit(train_loader, val_full_loader)"],"execution_count":null,"outputs":[]}]}